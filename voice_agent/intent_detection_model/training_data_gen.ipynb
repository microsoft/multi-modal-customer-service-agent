{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml  \n",
    "from typing import Any  \n",
    "import os  \n",
    "from sqlalchemy.ext.declarative import declarative_base  \n",
    "from sqlalchemy.orm import sessionmaker, relationship  \n",
    "from datetime import datetime  \n",
    "import random  \n",
    "from dotenv import load_dotenv  \n",
    "from openai import AsyncAzureOpenAI, AzureOpenAI \n",
    "from pathlib import Path  \n",
    "import json  \n",
    "from scipy import spatial  # for calculating vector similarities for search  \n",
    "import json  \n",
    "import matplotlib.pyplot as plt  \n",
    "from collections import Counter \n",
    "# Load YAML file  \n",
    "def load_entity(file_path, entity_name):  \n",
    "    with open(file_path, 'r') as file:  \n",
    "        data = yaml.safe_load(file)  \n",
    "    for entity in data['agents']:  \n",
    "        if entity.get('name') == entity_name:  \n",
    "            return entity  \n",
    "    return None  \n",
    "  \n",
    "# Load environment variables  \n",
    "env_path = Path('../app/backend') / '.env'  \n",
    "load_dotenv(dotenv_path=env_path)  \n",
    "client = AzureOpenAI(  \n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),  \n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),  \n",
    ")  \n",
    "chat_deployment=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT\")\n",
    "agents = {}\n",
    "with open('../app/backend/prompt.yaml', 'r') as file:  \n",
    "    data = yaml.safe_load(file)\n",
    "    for entity in data['agents']:  \n",
    "        if entity.get('domain_description'):\n",
    "            agents[entity.get('name')] = entity.get('domain_description')\n",
    "prompt = \"\"\"\n",
    "### Updated Prompt  \n",
    "  \n",
    "I am developing a model for intent change detection within customer service conversations. The goal is to identify when a customer's intent shifts to a different domain than the one currently being handled by the agent.  \n",
    "  \n",
    "### Information on all possible domains that agents can handle:  \n",
    "  \n",
    "- **hotel_agent**: \"Deal with hotel reservations, confirmations, changes, and general hotel policy questions.\"  \n",
    "- **flight_agent**: \"Deal with flight reservations, confirmations, changes, and general airline policy questions.\"  \n",
    "- **car_rental_agent**: \"Deal with car rental reservations, confirmations, changes, and general car rental policy questions.\"  \n",
    "- **general_agent**: \"Deal with general customer inquiries, complaints, and requests.\"  \n",
    "  \n",
    "To achieve this, the model requires training data that includes:  \n",
    "  \n",
    "Please generate labeled training data for this model. Each data point should include:  \n",
    "  \n",
    "- **Conversation Transcripts**: The last few turns of conversations between customers and agents. Note that the conversation might not start from the beginning, and the user's transcript might be streaming with partial outputs. The conversation should have around 3-5 turns. \n",
    "- **Current Domain**: The domain the current agent is addressing.  \n",
    "- **A label** indicating whether the customer's intent has shifted to a different domain or remains the same, with possible values being \"no_change\" or the name of the new domain.  \n",
    "  \n",
    "This data will be used to train the model to accurately detect intent changes in real-time customer service interactions.  \n",
    "This time, I need to generate a lot more examples in \"no_change\", \"general_agent\" and \"car_rental_agent\"  categories so focus only on these. \n",
    "### Example Training Data in JSON Format  \n",
    "  \n",
    "```json  \n",
    "{'training_data': [\n",
    "\n",
    "    {  \n",
    "        \"conversation_transcript\": [  \n",
    "            \"agent: How can I assist you with your flight today?\",  \n",
    "            \"user: yesuser: I want to..uh\",  \n",
    "            \"user: book a new flight\"  \n",
    "        ],  \n",
    "        \"current_domain\": \"flight_agent\",  \n",
    "        \"intent_shift\": \"no_change\"  \n",
    "    },  \n",
    "    {  \n",
    "        \"conversation_transcript\": [  \n",
    "            \"user: ...and I also need to rent a car for my trip.\",  \n",
    "            \"agent: I can help you with the car rental reservation.\",  \n",
    "            \"user: great, I'd like to book a car for next week.\"  \n",
    "        ],  \n",
    "        \"current_domain\": \"car_rental_agent\",  \n",
    "        \"intent_shift\": \"no_change\"  \n",
    "    },  \n",
    "    {  \n",
    "        \"conversation_transcript\": [  \n",
    "            \"agent: Certainly! Here are your flight details:\n",
    "- **Flight Number:** AA423\n",
    "- **Departure Airport:** Airport A\n",
    "- **Arrival Airport:** Airport B\n",
    "- **Departure Time:** 21:00 on November 19, 2022\n",
    "- **Arrival Time:** 23:00 on November 19, 2022\n",
    "- **Seat Number:** 12A\n",
    "- **Ticket Class:** Economy\n",
    "- **Gate:** G5\n",
    "- **Ticket Number:** 1602534303\",  \n",
    "            \"user: Thank you, can I change to a new date? .\"  \n",
    "        ],  \n",
    "        \"current_domain\": \"flight_agent\",  \n",
    "        \"intent_shift\": \"no_change\"  \n",
    "    }  \n",
    "]  \n",
    "}\n",
    "\"\"\"\n",
    "def generate_training_data():\n",
    "        \n",
    "\n",
    "    response = client.chat.completions.create( \n",
    "        model=chat_deployment,  \n",
    "        messages=[ {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.9,\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        \n",
    "    )  \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "output_data =[]\n",
    "#write code to generate training 1000 data points, for each call, validate that the json is valid with keys conversation_transcript, current_domain, intent_shift.\n",
    "for i in range(100):\n",
    "    data = generate_training_data()\n",
    "    try:\n",
    "        data = json.loads(data)\n",
    "        print(data)\n",
    "        for d in data['training_data']:\n",
    "            if \"conversation_transcript\" in d and \"current_domain\" in d and \"intent_shift\" in d and (d[\"intent_shift\"] in agents or d[\"intent_shift\"]==\"no_change\") and d[\"current_domain\"] in agents:\n",
    "                output_data.append(d)\n",
    "            else:\n",
    "                print(d)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Open the file in append mode  \n",
    "with open('training_data_v2.jsonl', mode='a') as file:  \n",
    "    for d in output_data:  \n",
    "        transcript = '\\n'.join(d['conversation_transcript'])  \n",
    "        conversation = f\"##current_domain:{d['current_domain']}\\n##conversation:\\n{transcript}\"  \n",
    "        json_line = {  \n",
    "            \"conversation\": conversation,  \n",
    "            \"intent_shift\": d[\"intent_shift\"]  \n",
    "        }  \n",
    "        file.write(json.dumps(json_line) + '\\n')  \n",
    "from collections import Counter  \n",
    "  \n",
    " \n",
    "  \n",
    "# Load data from the JSONL file  \n",
    "intent_shifts = []  \n",
    "  \n",
    "with open('training_data_v2.jsonl', 'r') as file:  \n",
    "    for line in file:  \n",
    "        data = json.loads(line)  \n",
    "        intent_shifts.append(data['intent_shift'])  \n",
    "  \n",
    "# Count the occurrences of each intent_shift  \n",
    "intent_shift_counts = Counter(intent_shifts)  \n",
    "  \n",
    "# Visualize the distribution  \n",
    "labels, values = zip(*intent_shift_counts.items())  \n",
    "  \n",
    "plt.figure(figsize=(10, 5))  \n",
    "plt.bar(labels, values, color='skyblue')  \n",
    "plt.xlabel('Intent Shift')  \n",
    "plt.ylabel('Frequency')  \n",
    "plt.title('Distribution of Intent Shifts')  \n",
    "plt.xticks(rotation=45)  \n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new version of training data with just detected intent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2602\n",
      "Test set size: 651\n"
     ]
    }
   ],
   "source": [
    "import json  \n",
    "  \n",
    "def process_jsonl(input_file, output_file):  \n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:  \n",
    "        for line in infile:  \n",
    "            # Load each line as a JSON object  \n",
    "            record = json.loads(line)  \n",
    "              \n",
    "            # Extract and remove the prefix from the \"conversation\" field  \n",
    "            conversation = record['conversation']  \n",
    "            if conversation.startswith(\"##current_domain:\"):  \n",
    "                end_of_prefix = conversation.find(\"\\n##conversation:\\n\") + len(\"\\n##conversation:\\n\")  \n",
    "                domain = conversation[len(\"##current_domain:\"):conversation.find(\"\\n\")]  \n",
    "                conversation = conversation[end_of_prefix:]  \n",
    "                record['conversation'] = conversation.strip()  \n",
    "                  \n",
    "                # Change the key \"intent_shift\" to \"intent\"  \n",
    "                record['intent'] = record.pop('intent_shift')  \n",
    "                  \n",
    "                # If \"intent\" is \"no_change\", update it to the extracted domain  \n",
    "                if record['intent'] == \"no_change\":  \n",
    "                    record['intent'] = domain  \n",
    "              \n",
    "            # Write the updated record back to the output file  \n",
    "            json.dump(record, outfile)  \n",
    "            outfile.write('\\n')  \n",
    "  \n",
    "# Call the function with the appropriate file paths  \n",
    "process_jsonl('training_data_v2.jsonl', 'all_training_data_v2.jsonl')  \n",
    "\n",
    "\n",
    "import json  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "  \n",
    "# Load data from the JSONL file  \n",
    "output_data = []  \n",
    "with open('all_training_data_v2.jsonl', 'r') as file:  \n",
    "    for line in file:  \n",
    "        output_data.append(json.loads(line))  \n",
    "  \n",
    "# Extract labels for stratification  \n",
    "labels = [d['intent'] for d in output_data]  \n",
    "  \n",
    "# Perform a stratified split  \n",
    "train, test = train_test_split(output_data, test_size=0.2, stratify=labels)  \n",
    "  \n",
    "# Verify the split  \n",
    "print(f\"Training set size: {len(train)}\")  \n",
    "print(f\"Test set size: {len(test)}\")  \n",
    "  \n",
    "# Write the train data to training_data.jsonl  \n",
    "with open('training_data_v2.jsonl', mode='w') as file:  \n",
    "    for d in train:  \n",
    "        file.write(json.dumps(d) + '\\n')  \n",
    "  \n",
    "# Write the test data to validation_data.jsonl  \n",
    "with open('validation_data_v2.jsonl', mode='w') as file:  \n",
    "    for d in test:  \n",
    "        file.write(json.dumps(d) + '\\n')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2602\n",
      "Test set size: 651\n"
     ]
    }
   ],
   "source": [
    "import json  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "  \n",
    "# Load data from the JSONL file  \n",
    "output_data = []  \n",
    "with open('training_data_v2.jsonl', 'r') as file:  \n",
    "    for line in file:  \n",
    "        output_data.append(json.loads(line))  \n",
    "  \n",
    "# Extract labels for stratification  \n",
    "labels = [d['intent_shift'] for d in output_data]  \n",
    "  \n",
    "# Perform a stratified split  \n",
    "train, test = train_test_split(output_data, test_size=0.2, stratify=labels)  \n",
    "  \n",
    "# Verify the split  \n",
    "print(f\"Training set size: {len(train)}\")  \n",
    "print(f\"Test set size: {len(test)}\")  \n",
    "  \n",
    "# Write the train data to training_data.jsonl  \n",
    "with open('training_data.jsonl', mode='w') as file:  \n",
    "    for d in train:  \n",
    "        file.write(json.dumps(d) + '\\n')  \n",
    "  \n",
    "# Write the test data to validation_data.jsonl  \n",
    "with open('validation_data.jsonl', mode='w') as file:  \n",
    "    for d in test:  \n",
    "        file.write(json.dumps(d) + '\\n')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create documents and label file  \n",
    "# os.makedirs(\"documents\", exist_ok=True)  \n",
    "# documents = []  \n",
    "  \n",
    "# for i, d in enumerate(output_data):  \n",
    "#     location = f\"documents/{str(i).zfill(3)}.txt\"  \n",
    "#     transscript = '\\n'.join(d['conversation_transcript'])\n",
    "#     conversation = f\"##current_domain:{d['current_domain']}\\n##conversation:\\n{transscript}\"  \n",
    "#     with open(location, 'w') as file:  \n",
    "#         file.write(conversation)  \n",
    "      \n",
    "#     category = d[\"intent_shift\"]  \n",
    "#     documents.append({  \n",
    "#         \"location\": f\"{str(i).zfill(3)}.txt\",  \n",
    "#         \"language\": \"en-us\",  \n",
    "#         \"class\": {  \n",
    "#             \"category\": category  \n",
    "#         }  \n",
    "#     })  \n",
    "  \n",
    "# label_data = {  \n",
    "#     \"projectFileVersion\": \"2022-05-01\",  \n",
    "#     \"stringIndexType\": \"Utf16CodeUnit\",  \n",
    "#     \"metadata\": {  \n",
    "#         \"projectName\": \"IntentChangeDetection\",  \n",
    "#         \"storageInputContainerName\": \"example-data\",  \n",
    "#         \"projectKind\": \"CustomSingleLabelClassification\",  \n",
    "#         \"description\": \"Intent change detection in customer service\",  \n",
    "#         \"language\": \"en\",  \n",
    "#         \"multilingual\": False,  \n",
    "#         \"settings\": {}  \n",
    "#     },  \n",
    "#     \"assets\": {  \n",
    "#         \"projectKind\": \"CustomSingleLabelClassification\",  \n",
    "#         \"classes\": [{\"category\": agent} for agent in agents.keys()] + [{\"category\": \"no_change\"}],  \n",
    "#         \"documents\": documents  \n",
    "#     }  \n",
    "# }  \n",
    "  \n",
    "# with open('documents/label_document.json', 'w') as file:  \n",
    "#     json.dump(label_data, file, indent=2)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
