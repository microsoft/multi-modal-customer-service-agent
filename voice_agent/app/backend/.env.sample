# Azure OpenAI settings
AZURE_OPENAI_ENDPOINT=https://<resource>.openai.azure.com/
AZURE_OPENAI_API_KEY=<your-api-key>
AZURE_OPENAI_API_VERSION=2025-03-01-preview
AZURE_OPENAI_REALTIME_DEPLOYMENT_NAME=gpt-4o-realtime-preview
AZURE_OPENAI_4O_MINI_DEPLOYMENT=<your-azure-openai-4o-mini-deployment-name>
AZURE_OPENAI_EMB_DEPLOYMENT=<your-text-embedding-ada-002-deployment-name> # must be model type: text-embedding-ada-002

# Optional: If you are using a different embedding endpoint, uncomment the lines below and provide the necessary values.
# AZURE_OPENAI_EMB_ENDPOINT= [Optional] if different from your realtime endpoint
# AZURE_OPENAI_EMB_API_KEY= [Optional] if providing an embedding endpoint

# If you are using a custom trained model for intent classification, uncomment the lines below and provide the necessary values.
# INTENT_SHIFT_API_KEY=<your-api-key>
# INTENT_SHIFT_API_URL=https://YOUR_ML_DEPLOYMENT.westus2.inference.ml.azure.com/score
# INTENT_SHIFT_API_DEPLOYMENT=YOUR_ML_DEPLOYMENT_NAME

# Voice settings
VOICE_NAME=shimmer
TRANSCRIPTION_MODEL=whisper-1
TURN_DETECTION_MODEL=server_vad
TURN_DETECTION_THRESHOLD=0.5
TURN_DETECTION_PREFIX_PADDING_MS=300
TURN_DETECTION_SILENCE_DURATION_MS=200

# Optional: If you want to use Redis for caching which supports distributed caching for high availability, uncomment the lines below and provide the necessary values.
# AZURE_REDIS_ENDPOINT=#optional, if you want to use redis for caching which support distributed caching for high
# AZURE_REDIS_KEY=#optional, if you want to use redis for caching which support distributed caching for high

# Observability and telemetry settings
ASPIRE_DASHBOARD_ENDPOINT=http://host.docker.internal:4317
TELEMETRY_SCENARIO=console